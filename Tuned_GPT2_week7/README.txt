generate_train_script.py
I have many text files about the medicinal herb Huangjing. I combine these files into one large text file to facilitate model training.

embedding.py
Next, I manually add a padding token and load the pre-trained GPT-2 model and tokenizer.

custom_dataset.py
Before feeding the data into the model, I tokenize and encode the data to ensure that each input sequence has a consistent length. If a sequence is too long, I truncate it; if it's too short, I pad it to the specified length.

train_tuned_gpt2_model.py
After preprocessing the data, I convert it into PyTorch's DataLoader format to make training the model more convenient. Additionally, I add labels to each input (a clone of the input sequence) so the model can compute the loss.

Model Fine-tuning
Then, I define the optimizer (such as AdamW) and the learning rate scheduler, and use PyTorch's training loop to fine-tune the model. Once the training is complete, I save the model and tokenizer. Later, if I need to generate text, I can directly load these saved models.

print_generated_script.py
Finally, I create a text generation pipeline using the fine-tuned model and tokenizer. By inputting a new herbal description, I can generate a new script that matches the style of the training data.

script_generated.txt
This is an example of the text generated given "冬季路边卖得最火的水果?当然是甘蔗啊!大街小巷，都能看见卖甘蔗的商贩，很多是推着木车，边走边卖。小编从小时就很喜欢甘蔗，咬上一口脆脆的甘蔗，吮吸其中甘甜的果蔗汁，立时觉得整个世界都甜了。"
Get: 冬季路边卖得最火的水果?当然是甘蔗啊!大街小巷，都能看见卖甘蔗的商贩，很多是推着木车，边走边卖。小编从小时就很喜欢甘蔗，咬上一口脆脆的甘蔗，吮吸其中甘甜的果蔗汁，立时觉得整个世界都甜了。 那 时 ， 小 编 就 是 那 个 真 的 的 人 。 那 时 ， 我 ， 你 ， 你 ， 你 ， 你 ， 你 ， 你 ， 你 ， 我 ， 我 ， 我 ， 我 ， 我 ， 我 ， 我 ， 甜 甜 甜 甜 的 甜 甜 甜 甜 甜 甜 的 甜 甜 甜 甜 甜 暖 暖 暖 暖 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒 寒

Evaluation
The result is not satisfied due to these reasons:
1. The training data is not diverse enough, so the model-generated text may lack creativity or become overly focused on certain patterns present in the training data.
2. The training data quantity is insufficient, the model may not learn enough information, resulting in lower quality generated text.
3. The training data contains noise since the scripts are generated by Whisper, so Chinese words are not correctly converted from recording to text.
 
For tranning data, I only use txts of HuangJing due to the long runtime. Despite that I only use about 50 txts, the training time takes about 5 hours. 
These are the main reasons why may cost so long:
1.GPT-2 is a very complex model with a large number of parameters, requiring substantial computational resources.
2. I am using a standard CPU instead of a GPU.
3. I have 3 training Epochs. While multiple epochs are typically needed to ensure the model converges, this also costs lots of time.